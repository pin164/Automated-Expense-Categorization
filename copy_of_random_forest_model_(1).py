# -*- coding: utf-8 -*-
"""Copy of Random_forest_model (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J6iOdpOxN4vhhia6WUKrE6WzQYbsr9tq

<a href="https://colab.research.google.com/github/pin164/Automated-Expense-Categorization/blob/main/Decision_Tree_model.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

# STEP 1
# Import  dependencies
!pip install category_encoders
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn import tree
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import Callback
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
import category_encoders as ce

from google.colab import drive
drive.mount('/content/drive')

"""## Preprocessing"""

# STEP 2
#  Import and read the Cleaned_Invoice_4.csv
#/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv
application_df = pd.read_csv("/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv",encoding='latin1').iloc[:,:14]
application_df.head()

# STEP 3
# extract beneficial columns only
####X_df = application_df[['Column2','PO#','TRS','Order#','Column3']]
X_df = application_df[['PO#','TRS','Order#']]
Y_df = application_df['Cost Centre']

#Step 4
# amend the data type of order to be an object
X_df.loc[:, 'Order#'] = X_df['Order#'].astype('object')

X_df.dtypes

#Step 5
# Ordinal encoding transforms the columns PO#','TRS','Order#'
# into a column of class integers.
# Ordinal encoding scheme maps columns to integers randomly


encoder = ce.OrdinalEncoder()
encoder.fit_transform(X_df).head()

X_encoded = encoder.fit_transform(X_df) # Store the encoded DataFrame in X_encoded

#Drop Rows with Missing Target Values:
#remove the rows from both the feature set X and the target set Y_df that contain missing target values.
# Combine X and y to drop rows with missing target values
combined_df = pd.concat([X_encoded, Y_df], axis=1)

# Drop rows where the target variable is missing
combined_df.dropna(subset=['Cost Centre'], inplace=True)

# Separate the features and target again after dropping missing values
X = combined_df.drop(columns=['Cost Centre']).values
y = combined_df['Cost Centre'].values


# Re-split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets to verify the split
print("Training Features Shape:", X_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Training Target Shape:", y_train.shape)
print("Testing Target Shape:", y_test.shape)

#  Step 6: Standardize (scale) the feature data
scaler = StandardScaler()

# Fit on training data, transform both training and test sets
X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)
X_test_scaled = scaler.transform(X_test).astype(np.float32)

from sklearn.ensemble import RandomForestClassifier

# Initialize the model
rf_model = RandomForestClassifier(random_state=42)



#Some Cost Centres dominate the dataset,
# the model could struggle to predict minority payees.
# Using Class Weights:
rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)
# fit the model
rf_model.fit(X_train, y_train)
# Prediction
y_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_pred, zero_division=1))



rf_confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(rf_confusion)

# clf is your trained RandomForestClassifier
import joblib
joblib.dump(rf_model, 'rf_model.pkl')

# Save encoder as well the way we saved the model
joblib.dump(encoder, 'category_encoder.pkl')

"""streamlit code"""

## create a def

import pandas as pd
import category_encoders as ce
from sklearn.ensemble import RandomForestClassifier
import joblib
# Load the model from the file
clf_loaded = joblib.load('/content/rf_model.pkl')
# Load the encoder from the file
encoder_loaded = joblib.load('/content/category_encoder.pkl')

clf_loaded

encoder_loaded

test =pd.DataFrame(columns= ['PO#','TRS','Order#'])

"""step one in streamlit"""

po = input("Enter PO#: ")
trs = input("Enter TRS: ")
order = input("Enter Order#: ")

test.loc[0] = [po,trs,order]

test

# Assuming you have new data (X_test) to transform
X_test_transformed = encoder_loaded.transform(test)

X_test_transformed

clf_loaded.predict(X_test_transformed)

# STEP 1
# Import  dependencies
!pip install category_encoders
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn import tree
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import Callback
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
import category_encoders as ce

from google.colab import drive
drive.mount('/content/drive')

"""## Preprocessing"""

# STEP 2
#  Import and read the Cleaned_Invoice_4.csv
#/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv
application_df = pd.read_csv("/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv",encoding='latin1').iloc[:,:14]
application_df.head()

# STEP 3
# extract beneficial columns only
####X_df = application_df[['Column2','PO#','TRS','Order#','Column3']]
X_df = application_df[['PO#','TRS','Order#']]
Y_df = application_df['Cost Centre']

#Step 4
# amend the data type of order to be an object
X_df.loc[:, 'Order#'] = X_df['Order#'].astype('object')

X_df.dtypes

#Step 5
# Ordinal encoding transforms the columns PO#','TRS','Order#'
# into a column of class integers.
# Ordinal encoding scheme maps columns to integers randomly


encoder = ce.OrdinalEncoder()
encoder.fit_transform(X_df).head()

X_encoded = encoder.fit_transform(X_df) # Store the encoded DataFrame in X_encoded

#Drop Rows with Missing Target Values:
#remove the rows from both the feature set X and the target set Y_df that contain missing target values.
# Combine X and y to drop rows with missing target values
combined_df = pd.concat([X_encoded, Y_df], axis=1)

# Drop rows where the target variable is missing
combined_df.dropna(subset=['Cost Centre'], inplace=True)

# Separate the features and target again after dropping missing values
X = combined_df.drop(columns=['Cost Centre']).values
y = combined_df['Cost Centre'].values


# Re-split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets to verify the split
print("Training Features Shape:", X_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Training Target Shape:", y_train.shape)
print("Testing Target Shape:", y_test.shape)

#  Step 6: Standardize (scale) the feature data
scaler = StandardScaler()

# Fit on training data, transform both training and test sets
X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)
X_test_scaled = scaler.transform(X_test).astype(np.float32)

from sklearn.ensemble import RandomForestClassifier

# Initialize the model
rf_model = RandomForestClassifier(random_state=42)



#Some Cost Centres dominate the dataset,
# the model could struggle to predict minority payees.
# Using Class Weights:
rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)
# fit the model
rf_model.fit(X_train, y_train)
# Prediction
y_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_pred, zero_division=1))



rf_confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(rf_confusion)

# clf is your trained RandomForestClassifier
import joblib
joblib.dump(rf_model, 'rf_model.pkl')

# Save encoder as well the way we saved the model
joblib.dump(encoder, 'category_encoder.pkl')

"""streamlit code"""

## create a def

import pandas as pd
import category_encoders as ce
from sklearn.ensemble import RandomForestClassifier
import joblib
# Load the model from the file
clf_loaded = joblib.load('/content/rf_model.pkl')
# Load the encoder from the file
encoder_loaded = joblib.load('/content/category_encoder.pkl')

clf_loaded

encoder_loaded

test =pd.DataFrame(columns= ['PO#','TRS','Order#'])

"""step one in streamlit"""

po = input("Enter PO#: ")
trs = input("Enter TRS: ")
order = input("Enter Order#: ")

test.loc[0] = [po,trs,order]

test

# Assuming you have new data (X_test) to transform
X_test_transformed = encoder_loaded.transform(test)

X_test_transformed

clf_loaded.predict(X_test_transformed)

# STEP 1
# Import  dependencies
!pip install category_encoders
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn import tree
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import Callback
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
import category_encoders as ce

from google.colab import drive
drive.mount('/content/drive')

"""## Preprocessing"""

# STEP 2
#  Import and read the Cleaned_Invoice_4.csv
#/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv
application_df = pd.read_csv("/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv",encoding='latin1').iloc[:,:14]
application_df.head()

# STEP 3
# extract beneficial columns only
####X_df = application_df[['Column2','PO#','TRS','Order#','Column3']]
X_df = application_df[['PO#','TRS','Order#']]
Y_df = application_df['Cost Centre']

#Step 4
# amend the data type of order to be an object
X_df.loc[:, 'Order#'] = X_df['Order#'].astype('object')

X_df.dtypes

#Step 5
# Ordinal encoding transforms the columns PO#','TRS','Order#'
# into a column of class integers.
# Ordinal encoding scheme maps columns to integers randomly


encoder = ce.OrdinalEncoder()
encoder.fit_transform(X_df).head()

X_encoded = encoder.fit_transform(X_df) # Store the encoded DataFrame in X_encoded

#Drop Rows with Missing Target Values:
#remove the rows from both the feature set X and the target set Y_df that contain missing target values.
# Combine X and y to drop rows with missing target values
combined_df = pd.concat([X_encoded, Y_df], axis=1)

# Drop rows where the target variable is missing
combined_df.dropna(subset=['Cost Centre'], inplace=True)

# Separate the features and target again after dropping missing values
X = combined_df.drop(columns=['Cost Centre']).values
y = combined_df['Cost Centre'].values


# Re-split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets to verify the split
print("Training Features Shape:", X_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Training Target Shape:", y_train.shape)
print("Testing Target Shape:", y_test.shape)

#  Step 6: Standardize (scale) the feature data
scaler = StandardScaler()

# Fit on training data, transform both training and test sets
X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)
X_test_scaled = scaler.transform(X_test).astype(np.float32)

from sklearn.ensemble import RandomForestClassifier

# Initialize the model
rf_model = RandomForestClassifier(random_state=42)



#Some Cost Centres dominate the dataset,
# the model could struggle to predict minority payees.
# Using Class Weights:
rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)
# fit the model
rf_model.fit(X_train, y_train)
# Prediction
y_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_pred, zero_division=1))



rf_confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(rf_confusion)

# clf is your trained RandomForestClassifier
import joblib
joblib.dump(rf_model, 'rf_model.pkl')

# Save encoder as well the way we saved the model
joblib.dump(encoder, 'category_encoder.pkl')

"""streamlit code"""

## create a def

import pandas as pd
import category_encoders as ce
from sklearn.ensemble import RandomForestClassifier
import joblib
# Load the model from the file
clf_loaded = joblib.load('/content/rf_model.pkl')
# Load the encoder from the file
encoder_loaded = joblib.load('/content/category_encoder.pkl')

clf_loaded

encoder_loaded

test =pd.DataFrame(columns= ['PO#','TRS','Order#'])

"""step one in streamlit"""

po = input("Enter PO#: ")
trs = input("Enter TRS: ")
order = input("Enter Order#: ")

test.loc[0] = [po,trs,order]

test

# Assuming you have new data (X_test) to transform
X_test_transformed = encoder_loaded.transform(test)

X_test_transformed

clf_loaded.predict(X_test_transformed)

# STEP 1
# Import  dependencies
!pip install category_encoders
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn import tree
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import Callback
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
import category_encoders as ce

from google.colab import drive
drive.mount('/content/drive')

"""## Preprocessing"""

# STEP 2
#  Import and read the Cleaned_Invoice_4.csv
#/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv
application_df = pd.read_csv("/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv",encoding='latin1').iloc[:,:14]
application_df.head()

# STEP 3
# extract beneficial columns only
####X_df = application_df[['Column2','PO#','TRS','Order#','Column3']]
X_df = application_df[['PO#','TRS','Order#']]
Y_df = application_df['Cost Centre']

#Step 4
# amend the data type of order to be an object
X_df.loc[:, 'Order#'] = X_df['Order#'].astype('object')

X_df.dtypes

#Step 5
# Ordinal encoding transforms the columns PO#','TRS','Order#'
# into a column of class integers.
# Ordinal encoding scheme maps columns to integers randomly


encoder = ce.OrdinalEncoder()
encoder.fit_transform(X_df).head()

X_encoded = encoder.fit_transform(X_df) # Store the encoded DataFrame in X_encoded

#Drop Rows with Missing Target Values:
#remove the rows from both the feature set X and the target set Y_df that contain missing target values.
# Combine X and y to drop rows with missing target values
combined_df = pd.concat([X_encoded, Y_df], axis=1)

# Drop rows where the target variable is missing
combined_df.dropna(subset=['Cost Centre'], inplace=True)

# Separate the features and target again after dropping missing values
X = combined_df.drop(columns=['Cost Centre']).values
y = combined_df['Cost Centre'].values


# Re-split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets to verify the split
print("Training Features Shape:", X_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Training Target Shape:", y_train.shape)
print("Testing Target Shape:", y_test.shape)

#  Step 6: Standardize (scale) the feature data
scaler = StandardScaler()

# Fit on training data, transform both training and test sets
X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)
X_test_scaled = scaler.transform(X_test).astype(np.float32)

from sklearn.ensemble import RandomForestClassifier

# Initialize the model
rf_model = RandomForestClassifier(random_state=42)



#Some Cost Centres dominate the dataset,
# the model could struggle to predict minority payees.
# Using Class Weights:
rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)
# fit the model
rf_model.fit(X_train, y_train)
# Prediction
y_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_pred, zero_division=1))



rf_confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(rf_confusion)

# clf is your trained RandomForestClassifier
import joblib
joblib.dump(rf_model, 'rf_model.pkl')

# Save encoder as well the way we saved the model
joblib.dump(encoder, 'category_encoder.pkl')

"""streamlit code"""

## create a def

import pandas as pd
import category_encoders as ce
from sklearn.ensemble import RandomForestClassifier
import joblib
# Load the model from the file
clf_loaded = joblib.load('/content/rf_model.pkl')
# Load the encoder from the file
encoder_loaded = joblib.load('/content/category_encoder.pkl')

clf_loaded

encoder_loaded

test =pd.DataFrame(columns= ['PO#','TRS','Order#'])

"""step one in streamlit"""

po = input("Enter PO#: ")
trs = input("Enter TRS: ")
order = input("Enter Order#: ")

test.loc[0] = [po,trs,order]

test

# Assuming you have new data (X_test) to transform
X_test_transformed = encoder_loaded.transform(test)

X_test_transformed

clf_loaded.predict(X_test_transformed)

# STEP 1
# Import  dependencies
!pip install category_encoders
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn import tree
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import Callback
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
import category_encoders as ce

from google.colab import drive
drive.mount('/content/drive')

"""## Preprocessing"""

# STEP 2
#  Import and read the Cleaned_Invoice_4.csv
#/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv
application_df = pd.read_csv("/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv",encoding='latin1').iloc[:,:14]
application_df.head()

# STEP 3
# extract beneficial columns only
####X_df = application_df[['Column2','PO#','TRS','Order#','Column3']]
X_df = application_df[['PO#','TRS','Order#']]
Y_df = application_df['Cost Centre']

#Step 4
# amend the data type of order to be an object
X_df.loc[:, 'Order#'] = X_df['Order#'].astype('object')

X_df.dtypes

#Step 5
# Ordinal encoding transforms the columns PO#','TRS','Order#'
# into a column of class integers.
# Ordinal encoding scheme maps columns to integers randomly


encoder = ce.OrdinalEncoder()
encoder.fit_transform(X_df).head()

X_encoded = encoder.fit_transform(X_df) # Store the encoded DataFrame in X_encoded

#Drop Rows with Missing Target Values:
#remove the rows from both the feature set X and the target set Y_df that contain missing target values.
# Combine X and y to drop rows with missing target values
combined_df = pd.concat([X_encoded, Y_df], axis=1)

# Drop rows where the target variable is missing
combined_df.dropna(subset=['Cost Centre'], inplace=True)

# Separate the features and target again after dropping missing values
X = combined_df.drop(columns=['Cost Centre']).values
y = combined_df['Cost Centre'].values


# Re-split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets to verify the split
print("Training Features Shape:", X_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Training Target Shape:", y_train.shape)
print("Testing Target Shape:", y_test.shape)

#  Step 6: Standardize (scale) the feature data
scaler = StandardScaler()

# Fit on training data, transform both training and test sets
X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)
X_test_scaled = scaler.transform(X_test).astype(np.float32)

from sklearn.ensemble import RandomForestClassifier

# Initialize the model
rf_model = RandomForestClassifier(random_state=42)



#Some Cost Centres dominate the dataset,
# the model could struggle to predict minority payees.
# Using Class Weights:
rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)
# fit the model
rf_model.fit(X_train, y_train)
# Prediction
y_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_pred, zero_division=1))



rf_confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(rf_confusion)

# clf is your trained RandomForestClassifier
import joblib
joblib.dump(rf_model, 'rf_model.pkl')

# Save encoder as well the way we saved the model
joblib.dump(encoder, 'category_encoder.pkl')

"""streamlit code"""

## create a def

import pandas as pd
import category_encoders as ce
from sklearn.ensemble import RandomForestClassifier
import joblib
# Load the model from the file
clf_loaded = joblib.load('/content/rf_model.pkl')
# Load the encoder from the file
encoder_loaded = joblib.load('/content/category_encoder.pkl')

clf_loaded

encoder_loaded

test =pd.DataFrame(columns= ['PO#','TRS','Order#'])

"""step one in streamlit"""

po = input("Enter PO#: ")
trs = input("Enter TRS: ")
order = input("Enter Order#: ")

test.loc[0] = [po,trs,order]

test

# Assuming you have new data (X_test) to transform
X_test_transformed = encoder_loaded.transform(test)

X_test_transformed

clf_loaded.predict(X_test_transformed)

# STEP 1
# Import  dependencies
!pip install category_encoders
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn import tree
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import Callback
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
import category_encoders as ce

from google.colab import drive
drive.mount('/content/drive')

"""## Preprocessing"""

# STEP 2
#  Import and read the Cleaned_Invoice_4.csv
#/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv
application_df = pd.read_csv("/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv",encoding='latin1').iloc[:,:14]
application_df.head()

# STEP 3
# extract beneficial columns only
####X_df = application_df[['Column2','PO#','TRS','Order#','Column3']]
X_df = application_df[['PO#','TRS','Order#']]
Y_df = application_df['Cost Centre']

#Step 4
# amend the data type of order to be an object
X_df.loc[:, 'Order#'] = X_df['Order#'].astype('object')

X_df.dtypes

#Step 5
# Ordinal encoding transforms the columns PO#','TRS','Order#'
# into a column of class integers.
# Ordinal encoding scheme maps columns to integers randomly


encoder = ce.OrdinalEncoder()
encoder.fit_transform(X_df).head()

X_encoded = encoder.fit_transform(X_df) # Store the encoded DataFrame in X_encoded

#Drop Rows with Missing Target Values:
#remove the rows from both the feature set X and the target set Y_df that contain missing target values.
# Combine X and y to drop rows with missing target values
combined_df = pd.concat([X_encoded, Y_df], axis=1)

# Drop rows where the target variable is missing
combined_df.dropna(subset=['Cost Centre'], inplace=True)

# Separate the features and target again after dropping missing values
X = combined_df.drop(columns=['Cost Centre']).values
y = combined_df['Cost Centre'].values


# Re-split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets to verify the split
print("Training Features Shape:", X_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Training Target Shape:", y_train.shape)
print("Testing Target Shape:", y_test.shape)

#  Step 6: Standardize (scale) the feature data
scaler = StandardScaler()

# Fit on training data, transform both training and test sets
X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)
X_test_scaled = scaler.transform(X_test).astype(np.float32)

from sklearn.ensemble import RandomForestClassifier

# Initialize the model
rf_model = RandomForestClassifier(random_state=42)



#Some Cost Centres dominate the dataset,
# the model could struggle to predict minority payees.
# Using Class Weights:
rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)
# fit the model
rf_model.fit(X_train, y_train)
# Prediction
y_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_pred, zero_division=1))



rf_confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(rf_confusion)

# clf is your trained RandomForestClassifier
import joblib
joblib.dump(rf_model, 'rf_model.pkl')

# Save encoder as well the way we saved the model
joblib.dump(encoder, 'category_encoder.pkl')

"""streamlit code"""

## create a def

import pandas as pd
import category_encoders as ce
from sklearn.ensemble import RandomForestClassifier
import joblib
# Load the model from the file
clf_loaded = joblib.load('/content/rf_model.pkl')
# Load the encoder from the file
encoder_loaded = joblib.load('/content/category_encoder.pkl')

clf_loaded

encoder_loaded

test =pd.DataFrame(columns= ['PO#','TRS','Order#'])

"""step one in streamlit"""

po = input("Enter PO#: ")
trs = input("Enter TRS: ")
order = input("Enter Order#: ")

test.loc[0] = [po,trs,order]

test

# Assuming you have new data (X_test) to transform
X_test_transformed = encoder_loaded.transform(test)

X_test_transformed

clf_loaded.predict(X_test_transformed)

# STEP 1
# Import  dependencies
!pip install category_encoders
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn import tree
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import Callback
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
import category_encoders as ce

from google.colab import drive
drive.mount('/content/drive')

"""## Preprocessing"""

# STEP 2
#  Import and read the Cleaned_Invoice_4.csv
#/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv
application_df = pd.read_csv("/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv",encoding='latin1').iloc[:,:14]
application_df.head()

# STEP 3
# extract beneficial columns only
####X_df = application_df[['Column2','PO#','TRS','Order#','Column3']]
X_df = application_df[['PO#','TRS','Order#']]
Y_df = application_df['Cost Centre']

#Step 4
# amend the data type of order to be an object
X_df.loc[:, 'Order#'] = X_df['Order#'].astype('object')

X_df.dtypes

#Step 5
# Ordinal encoding transforms the columns PO#','TRS','Order#'
# into a column of class integers.
# Ordinal encoding scheme maps columns to integers randomly


encoder = ce.OrdinalEncoder()
encoder.fit_transform(X_df).head()

X_encoded = encoder.fit_transform(X_df) # Store the encoded DataFrame in X_encoded

#Drop Rows with Missing Target Values:
#remove the rows from both the feature set X and the target set Y_df that contain missing target values.
# Combine X and y to drop rows with missing target values
combined_df = pd.concat([X_encoded, Y_df], axis=1)

# Drop rows where the target variable is missing
combined_df.dropna(subset=['Cost Centre'], inplace=True)

# Separate the features and target again after dropping missing values
X = combined_df.drop(columns=['Cost Centre']).values
y = combined_df['Cost Centre'].values


# Re-split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets to verify the split
print("Training Features Shape:", X_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Training Target Shape:", y_train.shape)
print("Testing Target Shape:", y_test.shape)

#  Step 6: Standardize (scale) the feature data
scaler = StandardScaler()

# Fit on training data, transform both training and test sets
X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)
X_test_scaled = scaler.transform(X_test).astype(np.float32)

from sklearn.ensemble import RandomForestClassifier

# Initialize the model
rf_model = RandomForestClassifier(random_state=42)



#Some Cost Centres dominate the dataset,
# the model could struggle to predict minority payees.
# Using Class Weights:
rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)
# fit the model
rf_model.fit(X_train, y_train)
# Prediction
y_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_pred, zero_division=1))



rf_confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(rf_confusion)

# clf is your trained RandomForestClassifier
import joblib
joblib.dump(rf_model, 'rf_model.pkl')

# Save encoder as well the way we saved the model
joblib.dump(encoder, 'category_encoder.pkl')

"""streamlit code"""

## create a def

import pandas as pd
import category_encoders as ce
from sklearn.ensemble import RandomForestClassifier
import joblib
# Load the model from the file
clf_loaded = joblib.load('/content/rf_model.pkl')
# Load the encoder from the file
encoder_loaded = joblib.load('/content/category_encoder.pkl')

clf_loaded

encoder_loaded

test =pd.DataFrame(columns= ['PO#','TRS','Order#'])

"""step one in streamlit"""

po = input("Enter PO#: ")
trs = input("Enter TRS: ")
order = input("Enter Order#: ")

test.loc[0] = [po,trs,order]

test

# Assuming you have new data (X_test) to transform
X_test_transformed = encoder_loaded.transform(test)

X_test_transformed

clf_loaded.predict(X_test_transformed)

# STEP 1
# Import  dependencies
!pip install category_encoders
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn import tree
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import Callback
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
import category_encoders as ce

from google.colab import drive
drive.mount('/content/drive')

"""## Preprocessing"""

# STEP 2
#  Import and read the Cleaned_Invoice_4.csv
#/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv
application_df = pd.read_csv("/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv",encoding='latin1').iloc[:,:14]
application_df.head()

# STEP 3
# extract beneficial columns only
####X_df = application_df[['Column2','PO#','TRS','Order#','Column3']]
X_df = application_df[['PO#','TRS','Order#']]
Y_df = application_df['Cost Centre']

#Step 4
# amend the data type of order to be an object
X_df.loc[:, 'Order#'] = X_df['Order#'].astype('object')

X_df.dtypes

#Step 5
# Ordinal encoding transforms the columns PO#','TRS','Order#'
# into a column of class integers.
# Ordinal encoding scheme maps columns to integers randomly


encoder = ce.OrdinalEncoder()
encoder.fit_transform(X_df).head()

X_encoded = encoder.fit_transform(X_df) # Store the encoded DataFrame in X_encoded

#Drop Rows with Missing Target Values:
#remove the rows from both the feature set X and the target set Y_df that contain missing target values.
# Combine X and y to drop rows with missing target values
combined_df = pd.concat([X_encoded, Y_df], axis=1)

# Drop rows where the target variable is missing
combined_df.dropna(subset=['Cost Centre'], inplace=True)

# Separate the features and target again after dropping missing values
X = combined_df.drop(columns=['Cost Centre']).values
y = combined_df['Cost Centre'].values


# Re-split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets to verify the split
print("Training Features Shape:", X_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Training Target Shape:", y_train.shape)
print("Testing Target Shape:", y_test.shape)

#  Step 6: Standardize (scale) the feature data
scaler = StandardScaler()

# Fit on training data, transform both training and test sets
X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)
X_test_scaled = scaler.transform(X_test).astype(np.float32)

from sklearn.ensemble import RandomForestClassifier

# Initialize the model
rf_model = RandomForestClassifier(random_state=42)



#Some Cost Centres dominate the dataset,
# the model could struggle to predict minority payees.
# Using Class Weights:
rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)
# fit the model
rf_model.fit(X_train, y_train)
# Prediction
y_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_pred, zero_division=1))



rf_confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(rf_confusion)

# clf is your trained RandomForestClassifier
import joblib
joblib.dump(rf_model, 'rf_model.pkl')

# Save encoder as well the way we saved the model
joblib.dump(encoder, 'category_encoder.pkl')

"""streamlit code"""

## create a def

import pandas as pd
import category_encoders as ce
from sklearn.ensemble import RandomForestClassifier
import joblib
# Load the model from the file
clf_loaded = joblib.load('/content/rf_model.pkl')
# Load the encoder from the file
encoder_loaded = joblib.load('/content/category_encoder.pkl')

clf_loaded

encoder_loaded

test =pd.DataFrame(columns= ['PO#','TRS','Order#'])

"""step one in streamlit"""

po = input("Enter PO#: ")
trs = input("Enter TRS: ")
order = input("Enter Order#: ")

test.loc[0] = [po,trs,order]

test

# Assuming you have new data (X_test) to transform
X_test_transformed = encoder_loaded.transform(test)

X_test_transformed

clf_loaded.predict(X_test_transformed)

# STEP 1
# Import  dependencies
!pip install category_encoders
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn import tree
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import Callback
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
import category_encoders as ce

from google.colab import drive
drive.mount('/content/drive')

"""## Preprocessing"""

# STEP 2
#  Import and read the Cleaned_Invoice_4.csv
#/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv
application_df = pd.read_csv("/content/drive/MyDrive/Project 4/Automated-Expense-Categorization/Cleaned_Invoice_4.csv",encoding='latin1').iloc[:,:14]
application_df.head()

# STEP 3
# extract beneficial columns only
####X_df = application_df[['Column2','PO#','TRS','Order#','Column3']]
X_df = application_df[['PO#','TRS','Order#']]
Y_df = application_df['Cost Centre']

#Step 4
# amend the data type of order to be an object
X_df.loc[:, 'Order#'] = X_df['Order#'].astype('object')

X_df.dtypes

#Step 5
# Ordinal encoding transforms the columns PO#','TRS','Order#'
# into a column of class integers.
# Ordinal encoding scheme maps columns to integers randomly


encoder = ce.OrdinalEncoder()
encoder.fit_transform(X_df).head()

X_encoded = encoder.fit_transform(X_df) # Store the encoded DataFrame in X_encoded

#Drop Rows with Missing Target Values:
#remove the rows from both the feature set X and the target set Y_df that contain missing target values.
# Combine X and y to drop rows with missing target values
combined_df = pd.concat([X_encoded, Y_df], axis=1)

# Drop rows where the target variable is missing
combined_df.dropna(subset=['Cost Centre'], inplace=True)

# Separate the features and target again after dropping missing values
X = combined_df.drop(columns=['Cost Centre']).values
y = combined_df['Cost Centre'].values


# Re-split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets to verify the split
print("Training Features Shape:", X_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Training Target Shape:", y_train.shape)
print("Testing Target Shape:", y_test.shape)

#  Step 6: Standardize (scale) the feature data
scaler = StandardScaler()

# Fit on training data, transform both training and test sets
X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)
X_test_scaled = scaler.transform(X_test).astype(np.float32)

from sklearn.ensemble import RandomForestClassifier

# Initialize the model
rf_model = RandomForestClassifier(random_state=42)



#Some Cost Centres dominate the dataset,
# the model could struggle to predict minority payees.
# Using Class Weights:
rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)
# fit the model
rf_model.fit(X_train, y_train)
# Prediction
y_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_pred, zero_division=1))



rf_confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(rf_confusion)

# clf is your trained RandomForestClassifier
import joblib
joblib.dump(rf_model, 'rf_model.pkl')

# Save encoder as well the way we saved the model
joblib.dump(encoder, 'category_encoder.pkl')

"""streamlit code"""

## create a def

import pandas as pd
import category_encoders as ce
from sklearn.ensemble import RandomForestClassifier
import joblib
# Load the model from the file
clf_loaded = joblib.load('/content/rf_model.pkl')
# Load the encoder from the file
encoder_loaded = joblib.load('/content/category_encoder.pkl')

clf_loaded

encoder_loaded

test =pd.DataFrame(columns= ['PO#','TRS','Order#'])

"""step one in streamlit"""

po = input("Enter PO#: ")
trs = input("Enter TRS: ")
order = input("Enter Order#: ")

test.loc[0] = [po,trs,order]

test

# Assuming you have new data (X_test) to transform
X_test_transformed = encoder_loaded.transform(test)

X_test_transformed

clf_loaded.predict(X_test_transformed)